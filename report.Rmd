---
title: "Practical Machine Learning Course Report"
author: "Pablo Adames"
date: "October 25, 2015"
output: html_document
---

# Summary

Generate a  model  to predict the type of  dumbell biceps curl 
executed by the subjects in the __Weight Lifting Exercises Dataset__
following the classification given by expert trainers. 


# Load data

Data source: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)

```{r, load data from disk, cache=TRUE, warning=FALSE}
# read the data for the project as training and test groups
trndat<-read.csv(file="data/pml-training.csv")
tstdat<-read.csv(file="data/pml-testing.csv")
```
```{r, output from loading data,cache=FALSE,echo=FALSE}
print(paste0("'trndat' data set: ",dim(trndat)[1], " observations", " on ", dim(trndat)[2]-1, " predictors."))
print(paste0("'tstdat' data set: ",dim(tstdat)[1], " observations", " on ", dim(tstdat)[2]-1, " predictors."))
```

# Cleaning data set

A first inspection showed there were many columns with 
non-numeric values. Discarding those columns removed 36 variables.
The `knnImpute` could not be used on this data set because of the
sensitivity of the time series. 

```{r, cleaning data part A,cache=TRUE,warning=FALSE,echo=FALSE,error=FALSE}
library(caret)
# get the column class
column_class <- function( col ){ifelse(is.data.frame(col), class(col[[1]]), class(col))}
extractNumericCols <- function( df ) {
  # get the column names to index inot the data frame
  l_col_names <- names(df)
  # nullify the columns with non-numeric values, return list of data.frames
  l <- sapply(l_col_names, 
              function(x) { cls <- column_class( df[x] );
              ifelse(cls %in% c("numeric","integer"), df[x], "NULL")})
  # filter the columns where there are only numeric values
  isNULL <- which(l[names(l)] =="NULL"); ll <- l[-isNULL] 
  # compose a data frame with the numeric columns (data.frames)
  ndf <- do.call(cbind.data.frame,ll)
}
# new data frame with only numeric columns:
trainSet <- extractNumericCols(trndat)
# add outcome column as column #1:
trainSet$classe <- trndat$classe
# filter the numeric columns in the test set
numericColNames <- colnames(trainSet)
testSet <- tstdat[,which(colnames(tstdat) %in% numericColNames)]
print(paste0("'trainSet' data set with ",dim(trainSet)[1], " observations", " on ", dim(trainSet)[2]-1, " predictors."))
```
The predictors with remaining `NA`s were eliminated
using the custom function `extractColumnsWithTooManyNAs` as shown in the
code shown in Appendix A.

```{r, cleaning data part B,cache=TRUE,warning=FALSE,echo=FALSE,error=FALSE}
#if there are still columns with NAs eliminate them altogether
extractColumnsWithTooManyNAs <- function( df){
  l <- sapply(colnames(df), function(x) any( is.na(df[x][[1]]) ) )
  df[-which(l)]
}
# eliminate all columns with NAs in them
trainClean <- extractColumnsWithTooManyNAs(trainSet)
trainCleanVars <- trainClean[-dim(trainClean)[2]]
classe <- trainClean[dim(trainClean)[2]]
testClean <- extractColumnsWithTooManyNAs(testSet)
print(paste0("'trainClean' data set with ",dim(trainClean)[1], " observations", " on ", dim(trainClean)[2]-1, " variables."))
print(paste0("'testClean' data set with ", dim(testClean)[1], " observations on ", dim(testClean)[2], " variables."))
```


# Exploratory analysys

Principal component analysis was used due to the high number of predictors.
A five component PCA shows that there are visible clusters for the PC1 and PC2 
interaction. There is only one outlier from this analysis. Further plots of PC1
Vs. PC2 show that only PC5 conditions the output in an important way.

```{r, exploratory analysis with PCA, cache=TRUE,warning=FALSE,fig.align='center'}
preproc <- preProcess(trainClean[-1],method = "pca",pcaComp = 5)
dumbbellPC <- predict(preproc,trainClean[-1])
featurePlot(x=dumbbellPC,y=trainClean$classe,plot="pairs")
```

# Generating a predictive model

The non-linear behaviour observed in the principal component analysis
motivated a random forest algorithm to be used for predicting the
type of the dumbbell bicep curl.
In view of the long time to fit a random forest model to this data set,
parallel processing was necessary.


```{r preparing parallel, cache=TRUE, echo=TRUE, warning=FALSE}
library(doParallel)
cl<-makeCluster(6)
registerDoParallel(cl)
require(randomForest)
```
This code takes a long time to run because it uses all 53 variables,
its final size is 2.9 Gb.
```
rf_model_full <- train(classe~.,data=trainClean,
                  method="rf",
                  trControl=trainControl(method="cv",number=5),
                  prox=TRUE,
                  allowParallel=TRUE)
rf_model_full
Random Forest 

19622 samples
   56 predictors
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 15698, 15698, 15698, 15697, 15697 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD   Kappa SD    
   2    0.9988788  0.9985818  0.0004966670  0.0006282220
  29    0.9998981  0.9998711  0.0001395827  0.0001765530
  56    0.9997961  0.9997421  0.0002132122  0.0002696789

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 29. 
```

Since the accuracy looks very high this model will be used to predict the
test samples.

```
answers <- predict(rf_model_full,newdata = testClean)
```

# Appendix A. Code for cleaning the original data set


```
library(caret)
# get the column class
column_class <- function( col ){ifelse(is.data.frame(col), class(col[[1]]), class(col))}
extractNumericCols <- function( df ) {
  # get the column names to index inot the data frame
  l_col_names <- names(df)
  # nullify the columns with non-numeric values, return list of data.frames
  l <- sapply(l_col_names, 
              function(x) { cls <- column_class( df[x] );
              ifelse(cls %in% c("numeric","integer"), df[x], "NULL")})
  # filter the columns where there are only numeric values
  isNULL <- which(l[names(l)] =="NULL"); ll <- l[-isNULL] 
  # compose a data frame with the numeric columns (data.frames)
  ndf <- do.call(cbind.data.frame,ll)
}
# new data frame with only numeric columns:
trainSet <- extractNumericCols(trndat)
# add outcome column as column #1:
trainSet$classe <- trndat$classe
# filter the numeric columns in the test set
numericColNames <- colnames(trainSet)
testSet <- tstdat[,which(colnames(tstdat) %in% numericColNames)]
print(paste0("'trainSet' data set with ",dim(trainSet)[1], " observations", " on ", dim(trainSet)[2]-1, " predictors."))

extractColumnsWithTooManyNAs <- function( df){
  l <- sapply(colnames(df), function(x) any( is.na(df[x][[1]]) ) )
  df[-which(l)]
}
# eliminate all columns with NAs in them
trainClean <- extractColumnsWithTooManyNAs(trainSet)
testClean <- extractColumnsWithTooManyNAs(testSet)
print(paste0("'trainClean' data set with ",dim(trainClean)[1], " observations", " on ", dim(trainClean)[2]-1, " variables."))
print(paste0("'testClean' data set with ", dim(testClean)[1], " observations on ", dim(testClean)[2], " variables."))

```


